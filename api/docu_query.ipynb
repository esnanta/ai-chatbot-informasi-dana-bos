{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B9lCgwJuNeHs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59f0783c24624f4e952df054fa09ef21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e859163675543e7bb7bd748ea97bcd8",
              "IPY_MODEL_6eb6fd416ab242189f56bede49448216",
              "IPY_MODEL_029004c6fa924164acc1cb3f9c5d1ff2"
            ],
            "layout": "IPY_MODEL_22749f38cd2f4d9483d6d785e7377c5f"
          }
        },
        "2e859163675543e7bb7bd748ea97bcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665513aaf5f84a9f9b4dafce7446e9bd",
            "placeholder": "​",
            "style": "IPY_MODEL_f7e4b666ad7d4345a7bd33037da4ed05",
            "value": "Batches: 100%"
          }
        },
        "6eb6fd416ab242189f56bede49448216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a95df049c1bf42b1baf6c0bfffd60493",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77a2e7f008084167bd79b006a6073274",
            "value": 2
          }
        },
        "029004c6fa924164acc1cb3f9c5d1ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df3cdfada1db4367a2f196f9d1f8e8f8",
            "placeholder": "​",
            "style": "IPY_MODEL_0180bf6d1e7f4ecc9bc41ddbb9193d1d",
            "value": " 2/2 [00:07&lt;00:00,  3.59s/it]"
          }
        },
        "22749f38cd2f4d9483d6d785e7377c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665513aaf5f84a9f9b4dafce7446e9bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e4b666ad7d4345a7bd33037da4ed05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a95df049c1bf42b1baf6c0bfffd60493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77a2e7f008084167bd79b006a6073274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df3cdfada1db4367a2f196f9d1f8e8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0180bf6d1e7f4ecc9bc41ddbb9193d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "1. This system that relies on semantic similarity. It finds the text in the document that is most similar to the user's question.\n",
        "2. If the user's question doesn't closely resemble the way the information is expressed in the document, the system may not find the correct answer.\n",
        "\n"
      ],
      "metadata": {
        "id": "B9lCgwJuNeHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Development\n",
        "1. Clarifying Expectation, example :\n",
        "    * Chatbot: \"Dana BOS digunakan untuk membiayai kegiatan operasional sekolah. Apakah Anda ingin mengetahui contoh kegiatan operasional yang dapat dibiayai oleh Dana BOS?\"\n",
        "2. Provide a list of example questions that the user can ask. This shows them the types of questions the chatbot is good at answering. Example:\n",
        "    * Apa saja syarat pengajuan Dana BOS?\n",
        "    * Bagaimana cara melaporkan penggunaan Dana BOS?\n",
        "    * Sebutkan contoh kegiatan yang dapat dibiayai oleh Dana BOS.\n",
        "3. Keyword Suggestions: As the user types their question, suggest relevant keywords that they can include to make their question more specific.\n",
        "4. Intent Recognition (Advanced): Implement a simple intent recognition system. This would analyze the user's question and try to identify the intent behind it (e.g., \"find allowed uses,\" \"find reporting requirements\"). Based on the intent, the chatbot could automatically rephrase the question to be more targeted. This requires more advanced natural language processing techniques.\n",
        "5. Expand the Training Data (If Possible): If you have the ability to add more data to the system, try to find documents that explicitly list the allowed uses of Dana BOS in a clear and structured way. This will make it easier for the semantic search to find the right information.\n",
        "6. Hybrid Approach (Advanced): Consider combining this semantic search approach with a more traditional keyword-based search. If the semantic search fails to find a good answer, the chatbot could fall back to a keyword search to find any relevant documents and present them to the user."
      ],
      "metadata": {
        "id": "1uA6_DXcA7dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "uMo4iKw2NpWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omTDcRywLsNw",
        "outputId": "4cee6d3d-e4c6-4ea7-933d-f73e118a0e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.50.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.29.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.47)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf nltk sentence-transformers langchain scikit-learn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "import logging\n",
        "import requests\n",
        "import fitz\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util # Import util untuk semantic_search\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # Import splitter baru\n",
        "\n",
        "# Download resource NLTK\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'punkt'...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Gathering"
      ],
      "metadata": {
        "id": "8SFcHCpwORjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1. DATA GATHERING\n",
        "# ===============================\n",
        "\n",
        "GITHUB_RAW_URL = \"https://raw.githubusercontent.com/esnanta/ai-chatbot-dana-bos-api/main/knowledge_base/\"\n",
        "FILES = [\n",
        "    \"Permendikbudriset_No_63_Tahun_2023.pdf\",\n",
        "]\n",
        "\n",
        "# Direktori penyimpanan di Colab\n",
        "pdf_dir = \"/content/knowledge_base\"\n",
        "os.makedirs(pdf_dir, exist_ok=True)\n",
        "\n",
        "# Download file PDF dari GitHub\n",
        "for file in FILES:\n",
        "    file_url = GITHUB_RAW_URL + file\n",
        "    try:\n",
        "        response = requests.get(file_url)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(os.path.join(pdf_dir, file), \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"✅ Berhasil mengunduh: {file}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Gagal mengunduh {file}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Kesalahan tak terduga saat mengunduh {file}: {e}\")\n",
        "\n",
        "\n",
        "# Cek file yang telah diunduh\n",
        "print(f\"Daftar file di {pdf_dir}: {os.listdir(pdf_dir)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUAjLu-hOUiX",
        "outputId": "efe45d06-5262-44b3-c209-56afd7857825"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Berhasil mengunduh: Permendikbudriset_No_63_Tahun_2023.pdf\n",
            "Daftar file di /content/knowledge_base: ['chunks.json', 'Permendikbudriset_No_63_Tahun_2023.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 2. EKSTRAKSI TEKS DARI FILE PDF\n",
        "# ===============================\n",
        "\n",
        "# --- PDF Text Extraction ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:  # Use context manager for safety\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text(\"text\") + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Gagal mengekstrak teks dari {pdf_path}: {e}\")\n",
        "\n",
        "\n",
        "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
        "pdf_texts = {}\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
        "    try:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        pdf_texts[pdf_file] = text\n",
        "        print(f\"✅ Berhasil mengekstrak teks dari: {pdf_file}\")\n",
        "    except RuntimeError as re:\n",
        "        print(f\"❌ Kesalahan saat ekstraksi: {re}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Kesalahan tidak terduga pada {pdf_file}: {e}\")\n",
        "\n",
        "\n",
        "# Cek hasil ekstraksi\n",
        "print(f\"\\nTotal file yang berhasil diekstrak: {len(pdf_texts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfX2D-2Qfsf",
        "outputId": "762e52d0-d67b-4427-c931-9bcc03edb9bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Berhasil mengekstrak teks dari: Permendikbudriset_No_63_Tahun_2023.pdf\n",
            "\n",
            "Total file yang berhasil diekstrak: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "AQ-aw2LwUTW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 3. PREPROCESSING TEKS\n",
        "# (Fungsi clean_text Anda dipertahankan)\n",
        "# ===============================\n",
        "def clean_text(text):\n",
        "    # This collapses multiple consecutive blank lines into a single blank line,\n",
        "    # reducing unnecessary whitespace.\n",
        "    text = re.sub(r'\\n+', '\\n', text) # Ganti \\n+ -> \\n dulu\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Baru ganti whitespace -> spasi\n",
        "\n",
        "    # Pasal 17. -> Pasal 17 (spasi)\n",
        "    text = re.sub(r'Pasal (\\d+)\\.(?=\\s|$)', r'Pasal \\1 ', text) # Lebih robust: . diikuti spasi atau akhir string\n",
        "\n",
        "    # Ayat (1a). -> Ayat (1a) (spasi)\n",
        "    text = re.sub(r'Ayat \\((\\d+[a-z]?)\\)\\.(?=\\s|$)', r'Ayat (\\1) ', text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.IGNORECASE)  # Remove URLs\n",
        "    text = re.sub(r'jdih\\.kemdikbud\\.go\\.id', '', text, flags=re.IGNORECASE)  # Remove specific website\n",
        "\n",
        "    # Replace page number pattern '- 4 -' with '(page 4)'\n",
        "    text = re.sub(r'\\s-\\s(\\d+)\\s-\\s', r' (page \\1) ', text) # Tetap berguna\n",
        "\n",
        "    # Hapus pola \"angka titik\" di akhir kalimat yang mungkin terlewat\n",
        "    # contoh: \"kalimat selesai 1.\" -> \"kalimat selesai\"\n",
        "    text = re.sub(r'(\\w)\\s+(\\d+)\\.\\s*$', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"Cleaning texts...\")\n",
        "cleaned_texts = {pdf: clean_text(text) for pdf, text in pdf_texts.items()}\n",
        "print(f\"Cleaned {len(cleaned_texts)} documents.\")"
      ],
      "metadata": {
        "id": "KHaUz08k3Fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a683c6c6-f032-4631-c9fd-a28be6b01c8f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning texts...\n",
            "Cleaned 1 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 4. CHUNKING TEKS (Menggunakan RecursiveCharacterTextSplitter)\n",
        "# ===============================\n",
        "print(\"Chunking texts...\")\n",
        "\n",
        "# Inisialisasi Text Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=700,      # Ukuran chunk dalam karakter (eksperimen dengan nilai ini)\n",
        "    chunk_overlap=150,    # Jumlah overlap antar chunk (eksperimen)\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"], # Pemisah prioritas\n",
        ")\n",
        "\n",
        "# --- Fungsi tambahan untuk post-processing chunk (opsional tapi disarankan) ---\n",
        "def post_process_chunk(chunk):\n",
        "    \"\"\"Membersihkan chunk setelah splitting.\"\"\"\n",
        "    # Hapus angka/bullet/simbol di awal chunk (setelah splitting mungkin muncul lagi)\n",
        "    chunk = re.sub(r'^\\s*[\\d\\-\\•\\*\\.]+\\s*', '', chunk)\n",
        "    # Hapus pola nomor halaman yang mungkin masih ada di tengah\n",
        "    chunk = re.sub(r'\\s*\\(page \\d+\\)\\s*', ' ', chunk)\n",
        "     # Hapus pola pasal/ayat yang mungkin berdiri sendiri tanpa konteks berarti setelah split\n",
        "    chunk = re.sub(r'^\\s*(Pasal \\d+[a-z]?|Ayat \\(\\d+[a-z]?\\))\\s*$', '', chunk, flags=re.IGNORECASE)\n",
        "    # Hilangkan spasi berlebihan yang mungkin muncul lagi\n",
        "    chunk = re.sub(r'\\s+', ' ', chunk).strip()\n",
        "    return chunk\n",
        "\n",
        "# --- Fungsi filter chunk yang terlalu pendek atau tidak relevan ---\n",
        "def filter_short_or_irrelevant(chunk, min_length=50):\n",
        "    \"\"\"Menghapus chunk yang terlalu pendek atau hanya berisi noise.\"\"\"\n",
        "    if len(chunk) < min_length:\n",
        "        return False\n",
        "    # Contoh pola tidak relevan (bisa ditambahkan sesuai kebutuhan)\n",
        "    if re.match(r'^\\s*(daftar isi|kata pengantar|lampiran)\\s*$', chunk, re.IGNORECASE):\n",
        "        return False\n",
        "    # Cek apakah chunk hanya berisi spasi atau karakter non-alfanumerik\n",
        "    if not re.search(r'[a-zA-Z0-9]', chunk):\n",
        "         return False\n",
        "    return True\n",
        "\n",
        "# Proses chunking untuk semua teks\n",
        "all_raw_chunks = []\n",
        "for pdf, text in cleaned_texts.items():\n",
        "    # Gunakan text_splitter untuk memecah teks\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    all_raw_chunks.extend(chunks)\n",
        "\n",
        "# Post-process dan filter chunks\n",
        "processed_chunks = [post_process_chunk(chunk) for chunk in all_raw_chunks]\n",
        "final_chunks = [chunk for chunk in processed_chunks if filter_short_or_irrelevant(chunk)]\n",
        "\n",
        "# Ganti nama variabel agar konsisten\n",
        "all_chunks = final_chunks # final_chunks sekarang menjadi data utama kita\n",
        "\n",
        "print(f\"Total final chunks after processing and filtering: {len(all_chunks)}\")\n",
        "# Optional: Tampilkan beberapa contoh chunk\n",
        "# for i, chunk in enumerate(all_chunks[:3]):\n",
        "#      print(f\"Chunk {i+1}:\\n{chunk}\\n-------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka9AdmeJHERa",
        "outputId": "7ee298db-e81b-4631-f131-539bc331e950"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking texts...\n",
            "Total final chunks after processing and filtering: 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING DATA #1\n",
        "\n",
        "* Chunk File\n",
        "* Cleaned Texts File"
      ],
      "metadata": {
        "id": "tNdrpdq1VEse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 5. SAVING CHUNKS\n",
        "# ===============================\n",
        "chunks_file = os.path.join(pdf_dir, \"chunks.json\")\n",
        "# cleaned_texts_file = os.path.join(pdf_dir, \"cleaned_texts.json\") # Opsional, jika masih perlu\n",
        "\n",
        "print(f\"Saving {len(all_chunks)} final chunks...\")\n",
        "try:\n",
        "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Final chunks saved to: {chunks_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving final chunks: {e}\")\n",
        "\n",
        "# Hapus penyimpanan cleaned_texts jika tidak diperlukan lagi\n",
        "# try:\n",
        "#     with open(cleaned_texts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#         json.dump(cleaned_texts, f, ensure_ascii=False, indent=4)\n",
        "#     print(f\"Cleaned texts saved to: {cleaned_texts_file}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error saving cleaned texts: {e}\")"
      ],
      "metadata": {
        "id": "uawPCJjiBhbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984faf5b-ab3b-481d-ea00-00ed8e1eb2c1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 53 final chunks...\n",
            "Final chunks saved to: /content/knowledge_base/chunks.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD MODEL"
      ],
      "metadata": {
        "id": "_TRngQpOU0vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 6. MODEL EMBEDDING (Hanya SentenceTransformer)\n",
        "# ===============================\n",
        "print(\"Loading Sentence Transformer model...\")\n",
        "# Hapus loading CrossEncoder\n",
        "# Pilih model embedder Anda\n",
        "# model_name = \"paraphrase-MiniLM-L3-v2\" # Model yang Anda coba\n",
        "model_name = \"all-MiniLM-L6-v2\" # Model yang lebih umum dan seringkali lebih baik (jika RAM cukup)\n",
        "# model_name = \"multi-qa-MiniLM-L6-cos-v1\" # Alternatif bagus untuk Q&A (jika RAM cukup)\n",
        "print(f\"Using model: {model_name}\")\n",
        "print(\"IMPORTANT: Ensure your pre-computed embeddings (if any) were created with THIS exact model!\")\n",
        "\n",
        "try:\n",
        "    embedder = SentenceTransformer(model_name)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model {model_name}: {e}\")\n",
        "    # Mungkin hentikan eksekusi jika model gagal dimuat\n",
        "    raise\n",
        "\n",
        "# Dapatkan dimensi embedding dari model yang dimuat\n",
        "d = embedder.get_sentence_embedding_dimension()\n",
        "print(f\"Embedding dimension (d): {d}\")\n",
        "\n",
        "# Encode semua chunk teks\n",
        "print(f\"Encoding {len(all_chunks)} chunks... (This may take a while)\")\n",
        "# Pastikan all_chunks tidak kosong\n",
        "if not all_chunks:\n",
        "    raise ValueError(\"Cannot encode embeddings: 'all_chunks' list is empty. Check chunking process.\")\n",
        "\n",
        "chunk_embeddings = embedder.encode(all_chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "print(f\"Encoded {chunk_embeddings.shape[0]} chunks.\")\n",
        "\n",
        "# HAPUS SEMUA KODE TERKAIT FAISS INDEXING (NLIST, NPROBE, quantizer, index, training, adding)"
      ],
      "metadata": {
        "id": "PL7UayJwU2Za",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "59f0783c24624f4e952df054fa09ef21",
            "2e859163675543e7bb7bd748ea97bcd8",
            "6eb6fd416ab242189f56bede49448216",
            "029004c6fa924164acc1cb3f9c5d1ff2",
            "22749f38cd2f4d9483d6d785e7377c5f",
            "665513aaf5f84a9f9b4dafce7446e9bd",
            "f7e4b666ad7d4345a7bd33037da4ed05",
            "a95df049c1bf42b1baf6c0bfffd60493",
            "77a2e7f008084167bd79b006a6073274",
            "df3cdfada1db4367a2f196f9d1f8e8f8",
            "0180bf6d1e7f4ecc9bc41ddbb9193d1d"
          ]
        },
        "outputId": "650578aa-8546-4571-ef94-8d97c095e1c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Sentence Transformer model...\n",
            "Using model: all-MiniLM-L6-v2\n",
            "IMPORTANT: Ensure your pre-computed embeddings (if any) were created with THIS exact model!\n",
            "Embedding dimension (d): 384\n",
            "Encoding 53 chunks... (This may take a while)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59f0783c24624f4e952df054fa09ef21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 53 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING DATA EMBEDDING"
      ],
      "metadata": {
        "id": "Oly414rtLU3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 7. SAVE EMBEDDINGS (TANPA FAISS INDEX)\n",
        "# ===============================\n",
        "embedding_file = os.path.join(pdf_dir, \"chunk_embeddings.npy\")\n",
        "\n",
        "print(\"Saving embeddings...\")\n",
        "try:\n",
        "    np.save(embedding_file, chunk_embeddings)\n",
        "    print(f\"Embeddings saved to: {embedding_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn5Bhd9lAaOt",
        "outputId": "bdd2c805-0b99-48a7-b3b8-fe2f5bc6bd39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving embeddings...\n",
            "Embeddings saved to: /content/knowledge_base/chunk_embeddings.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD EMBEDDINGS"
      ],
      "metadata": {
        "id": "B7NVcr1mQwUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 8. LOAD EMBEDDINGS (TANPA FAISS INDEX)\n",
        "# ===============================\n",
        "print(\"Loading embeddings...\")\n",
        "# Load embeddings dari file\n",
        "if os.path.exists(embedding_file):\n",
        "    # Muat embedding yang *baru saja disimpan* atau dari proses sebelumnya\n",
        "    # Pastikan ini sesuai dengan model embedder yang dimuat di langkah 6!\n",
        "    chunk_embeddings = np.load(embedding_file)\n",
        "    print(f\"Loaded embeddings with shape {chunk_embeddings.shape} from: {embedding_file}\")\n",
        "    # Validasi sederhana: jumlah embedding harus cocok dengan jumlah chunk\n",
        "    if len(all_chunks) != chunk_embeddings.shape[0]:\n",
        "         print(f\"WARNING: Mismatch between loaded chunks ({len(all_chunks)}) and loaded embeddings ({chunk_embeddings.shape[0]}). Using loaded chunks count.\")\n",
        "         # Jika terjadi mismatch, mungkin gunakan chunk yang dimuat dari JSON saja\n",
        "         try:\n",
        "             with open(chunks_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                 all_chunks = json.load(f)\n",
        "             print(f\"Reloaded {len(all_chunks)} chunks from {chunks_file}\")\n",
        "             if len(all_chunks) != chunk_embeddings.shape[0]:\n",
        "                  raise ValueError(f\"CRITICAL ERROR: Mismatch persists between chunks ({len(all_chunks)}) and embeddings ({chunk_embeddings.shape[0]}) even after reloading chunks.\")\n",
        "         except Exception as e:\n",
        "             print(f\"Error reloading chunks: {e}\")\n",
        "             raise\n",
        "    # Validasi dimensi\n",
        "    if chunk_embeddings.shape[1] != d:\n",
        "         raise ValueError(f\"CRITICAL ERROR: Loaded embedding dimension ({chunk_embeddings.shape[1]}) does not match model dimension ({d}). Ensure embeddings were generated with model '{model_name}'.\")\n",
        "\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Embedding file {embedding_file} not found. Please run the encoding step (6) first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0xKvMRgQ3rA",
        "outputId": "15aaa7fd-4fe5-4d2b-d7a2-a46ccec756fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings...\n",
            "Loaded embeddings with shape (53, 384) from: /content/knowledge_base/chunk_embeddings.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer"
      ],
      "metadata": {
        "id": "z0vqnuUjRiyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 9. FUNGSI UTAMA Q&A (Hanya Embedder + Cosine Similarity)\n",
        "# ===============================\n",
        "\n",
        "# Hapus fungsi extract_keywords dan filter_chunks_by_keywords\n",
        "\n",
        "def answer_question(question: str,\n",
        "                    chunks: list,\n",
        "                    embeddings: np.ndarray,\n",
        "                    embedder_model: SentenceTransformer,\n",
        "                    top_n: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Answers a question using Sentence Transformer embeddings and cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        question: The user's question.\n",
        "        chunks: A list of text chunks (strings).\n",
        "        embeddings: A NumPy array of embeddings corresponding to the chunks.\n",
        "        embedder_model: The loaded SentenceTransformer model.\n",
        "        top_n: The number of top relevant chunks to return.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the concatenated top_n relevant chunks.\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 Answering question: {question}\")\n",
        "\n",
        "    # 1. Encode Pertanyaan\n",
        "    print(\"   Encoding question...\")\n",
        "    question_embedding = embedder_model.encode([question], convert_to_numpy=True)\n",
        "\n",
        "    # 2. Hitung Cosine Similarity & Cari Top K\n",
        "    # Menggunakan util.semantic_search yang efisien\n",
        "    print(f\"   Searching top {top_n} chunks using semantic search...\")\n",
        "    # `util.semantic_search` mengembalikan list of lists, masing-masing berisi dict\n",
        "    # [{'corpus_id': index, 'score': similarity_score}, ...]\n",
        "    hits = util.semantic_search(question_embedding, embeddings, top_k=top_n)\n",
        "\n",
        "    # Ambil hasil dari query pertama (karena kita hanya encode 1 pertanyaan)\n",
        "    hits = hits[0]\n",
        "\n",
        "    # 3. Ambil Teks Chunk yang Relevan\n",
        "    relevant_chunks = []\n",
        "    print(\"   Relevant chunks found:\")\n",
        "    for hit in hits:\n",
        "        chunk_index = hit['corpus_id']\n",
        "        similarity_score = hit['score']\n",
        "        if 0 <= chunk_index < len(chunks):\n",
        "            print(f\"     - Chunk Index: {chunk_index}, Score: {similarity_score:.4f}\")\n",
        "            relevant_chunks.append(chunks[chunk_index])\n",
        "        else:\n",
        "            print(f\"     - Warning: Invalid chunk index {chunk_index} found in search results.\")\n",
        "\n",
        "    if not relevant_chunks:\n",
        "        print(\"   -> No relevant chunks found.\")\n",
        "        return \"Maaf, saya tidak dapat menemukan informasi yang relevan dengan pertanyaan Anda.\"\n",
        "\n",
        "    # 4. Gabungkan Jawaban\n",
        "    # Anda bisa memilih format penggabungan, misal <br><br> untuk HTML atau \\n\\n\n",
        "    final_answer = \"\\n\\n\".join(relevant_chunks)\n",
        "    print(\"   -> Final answer constructed.\")\n",
        "    return final_answer\n",
        "\n",
        "def post_process_answer(answer: str) -> str:\n",
        "    \"\"\"\n",
        "    Membersihkan dan memformat jawaban mentah agar lebih mudah dibaca.\n",
        "    - Tokenisasi kalimat\n",
        "    - Hapus duplikasi kalimat\n",
        "    - Format sebagai bullet list\n",
        "    - Filter kalimat yang sangat pendek\n",
        "    \"\"\"\n",
        "    print(\"   Post-processing answer...\")\n",
        "    # Ganti pemisah chunk asli (\\n\\n) dengan spasi agar sent_tokenize bekerja lebih baik\n",
        "    # pada batas antar chunk.\n",
        "    text_for_tokenize = answer.replace(\"\\n\\n\", \" \")\n",
        "    sentences = sent_tokenize(text_for_tokenize)\n",
        "\n",
        "    # Hapus duplikasi sambil menjaga urutan (membutuhkan Python 3.7+)\n",
        "    unique_sentences = list(dict.fromkeys(sentences))\n",
        "\n",
        "    # Filter kalimat pendek dan format sebagai bullet list\n",
        "    bulleted_list = []\n",
        "    min_sentence_length = 15 # Tingkatkan sedikit threshold panjang minimal\n",
        "    for sentence in unique_sentences:\n",
        "        cleaned_sentence = sentence.strip()\n",
        "        # Filter tambahan: cek jika kalimat hanya berisi spasi atau non-alpha\n",
        "        if len(cleaned_sentence) > min_sentence_length and re.search(r'[a-zA-Z]', cleaned_sentence):\n",
        "             # Capitalize first letter of each sentence for consistency\n",
        "             formatted_sentence = cleaned_sentence[0].upper() + cleaned_sentence[1:]\n",
        "             bulleted_list.append(f\"* {formatted_sentence}\")\n",
        "\n",
        "    if not bulleted_list:\n",
        "         print(\"   -> No valid sentences remained after post-processing.\")\n",
        "         # Kembalikan jawaban asli jika post-processing menghapus semuanya\n",
        "         # atau pesan default\n",
        "         return answer if answer.strip() else \"Tidak ada informasi detail yang dapat ditampilkan.\"\n",
        "\n",
        "    final_output = \"\\n\".join(bulleted_list)\n",
        "    print(f\"   -> Post-processing complete. Generated {len(bulleted_list)} bullets.\")\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "WHrZjdeiMLUC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING\n",
        "Chatbot menampilkan mengambil dan menampilkan 3 chunk teks yang paling mirip dengan pertanyaan pengguna sebagai jawaban."
      ],
      "metadata": {
        "id": "jZ1hT4h7fCl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 10. TESTING CHATBOT\n",
        "# ===============================\n",
        "\n",
        "# Pastikan variabel yang diperlukan sudah ada dari langkah sebelumnya\n",
        "if 'all_chunks' in locals() and 'chunk_embeddings' in locals() and 'embedder' in locals():\n",
        "\n",
        "    print(\"\\n================ TESTING CHATBOT ================\")\n",
        "\n",
        "    # Contoh pertanyaan\n",
        "    test_questions = [\n",
        "        \"Apakah Dana BOSP dapat digunakan untuk pengembangan sumber daya manusia?\",\n",
        "        \"Untuk apa saja Dana BOS Kinerja dapat digunakan?\",\n",
        "        \"Kapan laporan realisasi penggunaan Dana BOSP harus disampaikan?\",\n",
        "        \"Sebutkan komponen penggunaan dana BOS Reguler\", # Contoh pertanyaan lain\n",
        "        \"Apa saja larangan penggunaan dana BOSP?\",     # Contoh pertanyaan lain\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        # Panggil fungsi answer_question dengan argumen yang benar\n",
        "        # Hapus 'index' dan 'cross_encoder_model', tambahkan 'chunk_embeddings'\n",
        "        raw_answer = answer_question(\n",
        "            question=question,\n",
        "            chunks=all_chunks,\n",
        "            embeddings=chunk_embeddings, # Gunakan embeddings NumPy\n",
        "            embedder_model=embedder,     # Berikan model embedder\n",
        "            top_n=3                      # Jumlah hasil yang diinginkan\n",
        "        )\n",
        "\n",
        "        # (Opsional) Post-process jawaban (jika fungsi post_process_answer masih ada)\n",
        "        # Jika Anda menghapus fungsi post_process_answer, gunakan raw_answer saja\n",
        "        if 'post_process_answer' in locals():\n",
        "             processed_answer = post_process_answer(raw_answer)\n",
        "        else:\n",
        "             processed_answer = raw_answer # Gunakan jawaban mentah jika tidak ada post-processing\n",
        "\n",
        "        # Cetak hasil\n",
        "        print(f\"\\n🔹 **Pertanyaan:** {question}\")\n",
        "        print(f\"🔸 **Jawaban:**\\n{processed_answer}\\n\")\n",
        "        print(\"-\" * 40) # Pemisah antar pertanyaan\n",
        "\n",
        "    print(\"================ END OF TEST ================\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: Cannot run chatbot test.\")\n",
        "    print(\"Required variables ('all_chunks', 'chunk_embeddings', 'embedder') not found.\")\n",
        "    print(\"Please ensure the loading/encoding steps (Sections 6, 8 in the modified script) have run successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAASzWSmey1I",
        "outputId": "7c6fa4f7-1559-4e96-fdd1-d3aa00f2334f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ TESTING CHATBOT ================\n",
            "\n",
            "🔍 Answering question: Apakah Dana BOSP dapat digunakan untuk pengembangan sumber daya manusia?\n",
            "   Encoding question...\n",
            "   Searching top 3 chunks using semantic search...\n",
            "   Relevant chunks found:\n",
            "     - Chunk Index: 40, Score: 0.7352\n",
            "     - Chunk Index: 35, Score: 0.7312\n",
            "     - Chunk Index: 29, Score: 0.6492\n",
            "   -> Final answer constructed.\n",
            "   Post-processing answer...\n",
            "   -> Post-processing complete. Generated 4 bullets.\n",
            "\n",
            "🔹 **Pertanyaan:** Apakah Dana BOSP dapat digunakan untuk pengembangan sumber daya manusia?\n",
            "🔸 **Jawaban:**\n",
            "* Pengembangan sumber daya manusia merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan penguatan sumber daya manusia dalam rangka pelaksanaan Program Sekolah Penggerak, seperti: 1) identifikasi, pemetaan potensi dan kebutuhan pelatihan; 2) penguatan pelatihan griyaan (in house training) di Satuan Pendidikan; 3) penguatan komunitas belajar bagi kepala Satuan Pendidikan dan pendidik; 4) pelatihan bersama komunitas belajar; 5) pelaksanaan diskusi terpumpun bersama dengan guru SD kelas awal; 6) peningkatan kapasitas literasi digital; dan/ atau 7) kegiatan lainnya yang relevan dalam rangka pelaksanaan pengembangan sumber daya manusia.\n",
            "* B Pengembangan sumber daya manusia merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan penguatan sumber daya manusia dalam rangka pelaksanaan Program Sekolah Penggerak, seperti: 1) identifikasi, pemetaan potensi dan kebutuhan pelatihan; 2) penguatan pelatihan griyaan (in house training) di Satuan PAUD; 3) penguatan komunitas belajar bagi kepala Satuan PAUD dan pendidik; 4) pelatihan bersama komunitas belajar; 5) pelaksanaan diskusi terpumpun bersama dengan guru SD kelas awal; 6) peningkatan kapasitas literasi digital; dan/ atau 7) kegiatan lainnya yang relevan dalam rangka pelaksanaan pengembangan sumber daya manusia.\n",
            "* B tanggal 31 Januari tahun anggaran berikutnya untuk laporan realisasi keseluruhan penggunaan Dana BOSP yang diterima dalam satu tahun anggaran.\n",
            "* Di antara Pasal 52 dan Pasal 53 disisipkan 1 (satu) Pasal, yakni Pasal 52a sehingga berbunyi sebagai berikut: Pasal 52a (1) Laporan realisasi penggunaan Dana BOSP tahun sebelumnya digunakan sebagai dasar penyaluran tahap I tahun berkenaan\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "🔍 Answering question: Untuk apa saja Dana BOS Kinerja dapat digunakan?\n",
            "   Encoding question...\n",
            "   Searching top 3 chunks using semantic search...\n",
            "   Relevant chunks found:\n",
            "     - Chunk Index: 34, Score: 0.6035\n",
            "     - Chunk Index: 33, Score: 0.5557\n",
            "     - Chunk Index: 9, Score: 0.5532\n",
            "   -> Final answer constructed.\n",
            "   Post-processing answer...\n",
            "   -> Post-processing complete. Generated 5 bullets.\n",
            "\n",
            "🔹 **Pertanyaan:** Untuk apa saja Dana BOS Kinerja dapat digunakan?\n",
            "🔸 **Jawaban:**\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja, Dana BOS Kinerja, dan Dana BOP Kesetaraan Kinerja.\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja Sekolah yang Melaksanakan Program Sekolah Penggerak a Kepala Biro Hukum Kementerian Pendidikan, Kebudayaan, Riset, dan Teknologi, ttd.\n",
            "* Ineke Indraswati NIP 197809262000122001 SALINAN LAMPIRAN I PERATURAN MENTERI PENDIDIKAN, KEBUDAYAAN, RISET, DAN TEKNOLOGI REPUBLIK INDONESIA NOMOR 63 TAHUN 2023 TENTANG PERUBAHAN ATAS PERATURAN MENTERI PENDIDIKAN, KEBUDAYAAN, RISET, DAN TEKNOLOGI NOMOR 63 TAHUN 2022 TENTANG PETUNJUK TEKNIS PENGELOLAAN DANA BANTUAN OPERASIONAL SATUAN PENDIDIKAN RINCIAN KOMPONEN PENGGUNAAN DANA BANTUAN OPERASIONAL SATUAN PENDIDIKAN B. Rincian Komponen Penggunaan Dana BOP PAUD Kinerja, Dana BOS Kinerja, dan Dana BOP Kesetaraan Kinerja.\n",
            "* Dana Bantuan Operasional Penyelenggaraan Pendidikan Anak Usia Dini Kinerja yang selanjutnya disebut Dana BOP PAUD Kinerja adalah Dana BOP PAUD yang digunakan untuk peningkatan mutu pendidikan Satuan Pendidikan yang menyelenggarakan pendidikan anak usia dini yang dinilai berkinerja baik.\n",
            "* Dana Bantuan Operasional Sekolah Kinerja yang selanjutnya disebut Dana BOS Kinerja adalah Dana BOS yang digunakan untuk peningkatan mutu pendidikan Satuan Pendidikan yang menyelenggarakan pendidikan dasar dan pendidikan menengah yang dinilai berkinerja baik.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "🔍 Answering question: Kapan laporan realisasi penggunaan Dana BOSP harus disampaikan?\n",
            "   Encoding question...\n",
            "   Searching top 3 chunks using semantic search...\n",
            "   Relevant chunks found:\n",
            "     - Chunk Index: 29, Score: 0.6672\n",
            "     - Chunk Index: 34, Score: 0.6272\n",
            "     - Chunk Index: 49, Score: 0.6225\n",
            "   -> Final answer constructed.\n",
            "   Post-processing answer...\n",
            "   -> Post-processing complete. Generated 4 bullets.\n",
            "\n",
            "🔹 **Pertanyaan:** Kapan laporan realisasi penggunaan Dana BOSP harus disampaikan?\n",
            "🔸 **Jawaban:**\n",
            "* Tanggal 31 Januari tahun anggaran berikutnya untuk laporan realisasi keseluruhan penggunaan Dana BOSP yang diterima dalam satu tahun anggaran.\n",
            "* Di antara Pasal 52 dan Pasal 53 disisipkan 1 (satu) Pasal, yakni Pasal 52a sehingga berbunyi sebagai berikut: Pasal 52a (1) Laporan realisasi penggunaan Dana BOSP tahun sebelumnya digunakan sebagai dasar penyaluran tahap I tahun berkenaan Rincian Komponen Penggunaan Dana BOP PAUD Kinerja, Dana BOS Kinerja, dan Dana BOP Kesetaraan Kinerja.\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja Sekolah yang Melaksanakan Program Sekolah Penggerak a 6.\n",
            "* Rincian Komponen Penggunaan Dana BOS Kinerja Sekolah yang Memiliki Kemajuan Terbaik a. Pembelajaran kurikulum merdeka merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan pembelajaran bagi Peserta Didik yang berorientasi pada penguatan kompetensi dan pengembangan karakter, seperti: 1) fasilitasi penguatan kompetensi dan pengembangan karakter; 2) fasilitasi evaluasi pembelajaran berbasis rapor pendidikan; dan/atau 3) kegiatan lainnya yang relevan yang mendukung pembelajaran kurikulum merdeka.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "🔍 Answering question: Sebutkan komponen penggunaan dana BOS Reguler\n",
            "   Encoding question...\n",
            "   Searching top 3 chunks using semantic search...\n",
            "   Relevant chunks found:\n",
            "     - Chunk Index: 34, Score: 0.7366\n",
            "     - Chunk Index: 49, Score: 0.6841\n",
            "     - Chunk Index: 51, Score: 0.6782\n",
            "   -> Final answer constructed.\n",
            "   Post-processing answer...\n",
            "   -> Post-processing complete. Generated 4 bullets.\n",
            "\n",
            "🔹 **Pertanyaan:** Sebutkan komponen penggunaan dana BOS Reguler\n",
            "🔸 **Jawaban:**\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja, Dana BOS Kinerja, dan Dana BOP Kesetaraan Kinerja.\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja Sekolah yang Melaksanakan Program Sekolah Penggerak a 6.\n",
            "* Rincian Komponen Penggunaan Dana BOS Kinerja Sekolah yang Memiliki Kemajuan Terbaik a. Pembelajaran kurikulum merdeka merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan pembelajaran bagi Peserta Didik yang berorientasi pada penguatan kompetensi dan pengembangan karakter, seperti: 1) fasilitasi penguatan kompetensi dan pengembangan karakter; 2) fasilitasi evaluasi pembelajaran berbasis rapor pendidikan; dan/atau 3) kegiatan lainnya yang relevan yang mendukung pembelajaran kurikulum merdeka.\n",
            "* Rincian Komponen Penggunaan Dana BOP Kesetaraan Kinerja Sekolah yang Memiliki Kemajuan Terbaik a. Pembelajaran kurikulum merdeka merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan pembelajaran bagi Peserta Didik yang berorientasi pada penguatan kompetensi dan pengembangan karakter, seperti: 1) fasilitasi penguatan kompetensi dan pengembangan karakter; 2) fasilitasi evaluasi pembelajaran berbasis rapor pendidikan; dan/atau 3) kegiatan lainnya yang relevan yang mendukung pembelajaran kurikulum merdeka.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "🔍 Answering question: Apa saja larangan penggunaan dana BOSP?\n",
            "   Encoding question...\n",
            "   Searching top 3 chunks using semantic search...\n",
            "   Relevant chunks found:\n",
            "     - Chunk Index: 34, Score: 0.6290\n",
            "     - Chunk Index: 29, Score: 0.6217\n",
            "     - Chunk Index: 49, Score: 0.5842\n",
            "   -> Final answer constructed.\n",
            "   Post-processing answer...\n",
            "   -> Post-processing complete. Generated 4 bullets.\n",
            "\n",
            "🔹 **Pertanyaan:** Apa saja larangan penggunaan dana BOSP?\n",
            "🔸 **Jawaban:**\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja, Dana BOS Kinerja, dan Dana BOP Kesetaraan Kinerja.\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja Sekolah yang Melaksanakan Program Sekolah Penggerak a tanggal 31 Januari tahun anggaran berikutnya untuk laporan realisasi keseluruhan penggunaan Dana BOSP yang diterima dalam satu tahun anggaran.\n",
            "* Di antara Pasal 52 dan Pasal 53 disisipkan 1 (satu) Pasal, yakni Pasal 52a sehingga berbunyi sebagai berikut: Pasal 52a (1) Laporan realisasi penggunaan Dana BOSP tahun sebelumnya digunakan sebagai dasar penyaluran tahap I tahun berkenaan 6.\n",
            "* Rincian Komponen Penggunaan Dana BOS Kinerja Sekolah yang Memiliki Kemajuan Terbaik a. Pembelajaran kurikulum merdeka merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan pembelajaran bagi Peserta Didik yang berorientasi pada penguatan kompetensi dan pengembangan karakter, seperti: 1) fasilitasi penguatan kompetensi dan pengembangan karakter; 2) fasilitasi evaluasi pembelajaran berbasis rapor pendidikan; dan/atau 3) kegiatan lainnya yang relevan yang mendukung pembelajaran kurikulum merdeka.\n",
            "\n",
            "----------------------------------------\n",
            "================ END OF TEST ================\n"
          ]
        }
      ]
    }
  ]
}