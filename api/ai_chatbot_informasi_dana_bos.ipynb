{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B9lCgwJuNeHs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "1. This system that relies on semantic similarity. It finds the text in the document that is most similar to the user's question.\n",
        "2. If the user's question doesn't closely resemble the way the information is expressed in the document, the system may not find the correct answer.\n",
        "3. Basic Functionality covers:\n",
        "    * Extract text from PDF documents.\n",
        "    * Perform semantic search to find relevant chunks of text.\n",
        "    * Clean the output to remove unwanted content.\n",
        "    * Provide an answer to the user's question (even if the answer is not always perfect).\n",
        "\n"
      ],
      "metadata": {
        "id": "B9lCgwJuNeHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Development\n",
        "1. Clarifying Expectation, example :\n",
        "    * Chatbot: \"Dana BOS digunakan untuk membiayai kegiatan operasional sekolah. Apakah Anda ingin mengetahui contoh kegiatan operasional yang dapat dibiayai oleh Dana BOS?\"\n",
        "2. Provide a list of example questions that the user can ask. This shows them the types of questions the chatbot is good at answering. Example:\n",
        "    * Apa saja syarat pengajuan Dana BOS?\n",
        "    * Bagaimana cara melaporkan penggunaan Dana BOS?\n",
        "    * Sebutkan contoh kegiatan yang dapat dibiayai oleh Dana BOS.\n",
        "3. Keyword Suggestions: As the user types their question, suggest relevant keywords that they can include to make their question more specific.\n",
        "4. Intent Recognition (Advanced): Implement a simple intent recognition system. This would analyze the user's question and try to identify the intent behind it (e.g., \"find allowed uses,\" \"find reporting requirements\"). Based on the intent, the chatbot could automatically rephrase the question to be more targeted. This requires more advanced natural language processing techniques.\n",
        "5. Expand the Training Data (If Possible): If you have the ability to add more data to the system, try to find documents that explicitly list the allowed uses of Dana BOS in a clear and structured way. This will make it easier for the semantic search to find the right information.\n",
        "6. Hybrid Approach (Advanced): Consider combining this semantic search approach with a more traditional keyword-based search. If the semantic search fails to find a good answer, the chatbot could fall back to a keyword search to find any relevant documents and present them to the user."
      ],
      "metadata": {
        "id": "1uA6_DXcA7dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "uMo4iKw2NpWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omTDcRywLsNw",
        "outputId": "38d4dfef-2b04-4938-a5bf-77e65c71fcd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "!pip install pymupdf nltk transformers sentence-transformers faiss-cpu\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import fitz\n",
        "import nltk\n",
        "import faiss\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "# Download resource NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Gathering"
      ],
      "metadata": {
        "id": "8SFcHCpwORjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1. DATA GATHERING\n",
        "# ===============================\n",
        "\n",
        "GITHUB_RAW_URL = \"https://raw.githubusercontent.com/esnanta/ai-chatbot-dana-bos-api/main/knowledge_base/\"\n",
        "FILES = [\n",
        "    \"Permendikbudriset_No_63_Tahun_2023.pdf\",\n",
        "]\n",
        "\n",
        "# Direktori penyimpanan di Colab\n",
        "pdf_dir = \"/content/pdf_files\"\n",
        "os.makedirs(pdf_dir, exist_ok=True)\n",
        "\n",
        "# Download file PDF dari GitHub\n",
        "for file in FILES:\n",
        "    file_url = GITHUB_RAW_URL + file\n",
        "    try:\n",
        "        response = requests.get(file_url)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(os.path.join(pdf_dir, file), \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"✅ Berhasil mengunduh: {file}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Gagal mengunduh {file}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Kesalahan tak terduga saat mengunduh {file}: {e}\")\n",
        "\n",
        "\n",
        "# Cek file yang telah diunduh\n",
        "print(f\"Daftar file di {pdf_dir}: {os.listdir(pdf_dir)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUAjLu-hOUiX",
        "outputId": "91eb9b9c-3357-4558-ce62-d9d7bb383505"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Berhasil mengunduh: Permendikbudriset_No_63_Tahun_2023.pdf\n",
            "Daftar file di /content/pdf_files: ['chunks.json', 'cleaned_texts.json', 'Permendikbudriset_No_63_Tahun_2023.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 2. EKSTRAKSI TEKS DARI FILE PDF\n",
        "# ===============================\n",
        "\n",
        "# --- PDF Text Extraction ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:  # Use context manager for safety\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text(\"text\") + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Gagal mengekstrak teks dari {pdf_path}: {e}\")\n",
        "\n",
        "\n",
        "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
        "pdf_texts = {}\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
        "    try:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        pdf_texts[pdf_file] = text\n",
        "        print(f\"✅ Berhasil mengekstrak teks dari: {pdf_file}\")\n",
        "    except RuntimeError as re:\n",
        "        print(f\"❌ Kesalahan saat ekstraksi: {re}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Kesalahan tidak terduga pada {pdf_file}: {e}\")\n",
        "\n",
        "\n",
        "# Cek hasil ekstraksi\n",
        "print(f\"\\nTotal file yang berhasil diekstrak: {len(pdf_texts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfX2D-2Qfsf",
        "outputId": "24ae8749-7586-4afc-fc2a-6a43983c63f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Berhasil mengekstrak teks dari: Permendikbudriset_No_63_Tahun_2023.pdf\n",
            "\n",
            "Total file yang berhasil diekstrak: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "AQ-aw2LwUTW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 3. PREPROCESSING TEKS\n",
        "# ===============================\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    # This collapses multiple consecutive blank lines into a single blank line,\n",
        "    # reducing unnecessary whitespace.\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "\n",
        "    # replaces sequences of spaces, tabs, or newlines with a single space,\n",
        "    # ensuring consistent spacing\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # This line finds instances like \"Pasal 17.\" and replaces them with\n",
        "    # \"Pasal 17 \". It removes the dot after the number and ensures\n",
        "    # there is space. This prevents the sentence tokenizer from incorrectly\n",
        "    # splitting \"Pasal 17.\" into two sentences. It's important to keep\n",
        "    # \"Pasal 17\" together as a single unit.\n",
        "    text = re.sub(r'Pasal (\\d+)\\.\\s', r'Pasal \\1 ', text)\n",
        "\n",
        "    # Remove dot, KEEP contents of parentheses\n",
        "    text = re.sub(r'Ayat \\((\\d+[a-z]?)\\)\\.\\s', r'Ayat (\\1) ', text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.IGNORECASE)  # Remove URLs\n",
        "    text = re.sub(r'jdih\\.kemdikbud\\.go\\.id', '', text, flags=re.IGNORECASE)  # Remove specific website\n",
        "\n",
        "    # Replace page number pattern '- 4 -' with '(page 4)'\n",
        "    text = re.sub(r'\\s-\\s(\\d+)\\s-\\s', r' (page \\1) ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_texts = {pdf: clean_text(text) for pdf, text in pdf_texts.items()}"
      ],
      "metadata": {
        "id": "KHaUz08k3Fbc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 4. CHUNKING TEKS\n",
        "# Splits text into smaller chunks.\n",
        "# ===============================\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            if len(current_chunk) > 100:  # Pastikan chunk tidak terlalu kecil\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if len(current_chunk) > 100:  # Pastikan chunk tidak terlalu kecil\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def clean_chunk(chunk):\n",
        "    # Hapus angka atau simbol yang berada di awal baris\n",
        "    chunk = re.sub(r'^\\s*[\\d\\-\\•]+', '', chunk)\n",
        "    # Hapus angka yang berdiri sendiri tanpa konteks\n",
        "    chunk = re.sub(r'\\s*\\d+\\s*$', '', chunk)\n",
        "    return chunk.strip()\n",
        "\n",
        "def filter_irrelevant_text(chunk):\n",
        "    irrelevant_patterns = [\n",
        "        r'\\(\\d+\\)\\s*Dihapus',  # \"(3) Dihapus\", \"(4) Dihapus\"\n",
        "        r'-\\d+-',  # \"-9-\", \"-11-\" (kemungkinan nomor halaman)\n",
        "        r'Pasal\\s*\\d+',  # \"Pasal 52a\" (jika tidak ada konteks)\n",
        "        r'^\\s*\\.\\s*$',  # Tanda titik yang berdiri sendiri\n",
        "    ]\n",
        "\n",
        "    for pattern in irrelevant_patterns:\n",
        "        chunk = re.sub(pattern, '', chunk)\n",
        "\n",
        "    # Hapus angka yang berdiri sendiri, kecuali yang ada dalam kurung ( ) atau { }\n",
        "    chunk = re.sub(r'\\b\\d+\\b(?![\\)}])', ' ', chunk)  # Hanya hapus angka yang tidak diikuti kurung\n",
        "\n",
        "    # Hilangkan spasi berlebihan\n",
        "    chunk = re.sub(r'\\s+', ' ', chunk).strip()\n",
        "\n",
        "    return chunk\n",
        "\n",
        "all_chunks = []\n",
        "for pdf, text in cleaned_texts.items():\n",
        "    chunks = chunk_text(text)  # 1️⃣ Chunking dulu\n",
        "    cleaned_chunks = [clean_chunk(chunk) for chunk in chunks]  # 2️⃣ Clean angka awal\n",
        "    final_chunks = [filter_irrelevant_text(chunk) for chunk in cleaned_chunks]  # 3️⃣ Hapus bagian tidak relevan\n",
        "    all_chunks.extend(final_chunks)\n",
        "\n",
        "print(f\"Total chunks: {len(all_chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka9AdmeJHERa",
        "outputId": "315449cb-5f60-4539-be65-0ca94b5839bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING DATA #1\n",
        "\n",
        "* Chunk File\n",
        "* Cleaned Texts File"
      ],
      "metadata": {
        "id": "tNdrpdq1VEse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 5. SAVING DATA\n",
        "# ===============================\n",
        "\n",
        "# Define file paths for saving data\n",
        "chunks_file = os.path.join(pdf_dir, \"chunks.json\")  # Path to save chunks\n",
        "cleaned_texts_file = os.path.join(pdf_dir, \"cleaned_texts.json\") # Path to save cleaned texts\n",
        "\n",
        "# --------------------------------------\n",
        "# 1. Saving the Chunks of Text\n",
        "# --------------------------------------\n",
        "try:\n",
        "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Chunks saved to: {chunks_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving chunks: {e}\")\n",
        "\n",
        "# --------------------------------------\n",
        "# 2. Saving the Cleaned PDF Texts\n",
        "# --------------------------------------\n",
        "try:\n",
        "    with open(cleaned_texts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cleaned_texts, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Cleaned texts saved to: {cleaned_texts_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving cleaned texts: {e}\")"
      ],
      "metadata": {
        "id": "uawPCJjiBhbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6296c317-693f-4c2b-bb22-b5daf43378f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks saved to: /content/pdf_files/chunks.json\n",
            "Cleaned texts saved to: /content/pdf_files/cleaned_texts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD MODEL"
      ],
      "metadata": {
        "id": "_TRngQpOU0vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 6. MODEL & FAISS INDEXING\n",
        "# ===============================\n",
        "\n",
        "# Load model\n",
        "cross_encoder_model = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-6\")\n",
        "embedder = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
        "\n",
        "# Encode semua chunk teks\n",
        "chunk_embeddings = embedder.encode(all_chunks, convert_to_numpy=True)\n",
        "\n",
        "d = 384  # Dimensi embedding dari model MiniLM\n",
        "\n",
        "# Cluster tidak boleh lebih dari jumlah data\n",
        "# Jika data lebih banyak, tetap gunakan 100 cluster.\n",
        "NLIST = min(100, len(chunk_embeddings) // 4)\n",
        "NPROBE = 10  # Jumlah cluster yang dicari saat query\n",
        "\n",
        "\n",
        "\n",
        "# Buat quantizer dan FAISS IVF index\n",
        "quantizer = faiss.IndexFlatL2(d)  # Quantizer untuk clustering\n",
        "index = faiss.IndexIVFFlat(quantizer, d, NLIST, faiss.METRIC_L2)\n",
        "\n",
        "# Training FAISS IVF dengan semua embeddings\n",
        "if not index.is_trained:\n",
        "    print(\"Training FAISS IVF index...\")\n",
        "    index.train(chunk_embeddings)\n",
        "\n",
        "# Tambahkan embeddings ke FAISS Index\n",
        "index.add(chunk_embeddings)\n",
        "print(f\"FAISS IVF Index siap dengan {index.ntotal} data.\")"
      ],
      "metadata": {
        "id": "PL7UayJwU2Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718dadd7-b242-4779-aed1-b725fe96094b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FAISS IVF index...\n",
            "FAISS IVF Index siap dengan 66 data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING DATA #2\n",
        "\n",
        "* Embedding File\n",
        "* Faiss Index_file"
      ],
      "metadata": {
        "id": "Oly414rtLU3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 7. SAVE EMBEDDINGS & INDEX\n",
        "# ===============================\n",
        "\n",
        "# Define file paths for saving data\n",
        "embedding_file = os.path.join(pdf_dir, \"chunk_embeddings.npy\")\n",
        "faiss_index_file = os.path.join(pdf_dir, \"faiss_index.bin\")\n",
        "\n",
        "# --------------------------------------\n",
        "# a. Saving the embedding as .npy\n",
        "# --------------------------------------\n",
        "try:\n",
        "    np.save(embedding_file, chunk_embeddings)\n",
        "    print(f\"Embedding saved to: {embedding_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")\n",
        "\n",
        "# --------------------------------------\n",
        "# b. Saving FAISS file\n",
        "# --------------------------------------\n",
        "try:\n",
        "    faiss.write_index(index, faiss_index_file)\n",
        "    print(f\"FAISS IVF index saved to: {faiss_index_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving FAISS index: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn5Bhd9lAaOt",
        "outputId": "8fdcd4a7-6f6a-4d25-ff49-2952fce68930"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding saved to: /content/pdf_files/chunk_embeddings.npy\n",
            "FAISS IVF index saved to: /content/pdf_files/faiss_index.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD EMBEDDINGS & INDEX"
      ],
      "metadata": {
        "id": "B7NVcr1mQwUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 8. LOAD EMBEDDINGS & INDEX\n",
        "# ===============================\n",
        "\n",
        "# Load embeddings dari file\n",
        "if os.path.exists(embedding_file):\n",
        "    chunk_embeddings = np.load(embedding_file)\n",
        "    print(f\"Loaded embeddings from: {embedding_file}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Embedding file {embedding_file} not found.\")\n",
        "\n",
        "# Load FAISS index dari file\n",
        "if os.path.exists(faiss_index_file):\n",
        "    index = faiss.read_index(faiss_index_file)\n",
        "    print(f\"Loaded FAISS index from: {faiss_index_file}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"FAISS index file {faiss_index_file} not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0xKvMRgQ3rA",
        "outputId": "5afd35fa-ddf0-494d-cd80-9a587338b7bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded embeddings from: /content/pdf_files/chunk_embeddings.npy\n",
            "Loaded FAISS index from: /content/pdf_files/faiss_index.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer"
      ],
      "metadata": {
        "id": "z0vqnuUjRiyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 9. FUNGSI UTILITAS\n",
        "# ===============================\n",
        "\n",
        "# Fungsi untuk ekstraksi kata kunci tanpa stopwords\n",
        "def extract_keywords(question, top_n=5):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([question])\n",
        "\n",
        "    if tfidf_matrix.shape[1] == 0:\n",
        "        return set()\n",
        "\n",
        "    feature_array = np.array(vectorizer.get_feature_names_out())\n",
        "    tfidf_sorting = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n",
        "\n",
        "    return set(feature_array[tfidf_sorting][:top_n])\n",
        "\n",
        "# Fungsi untuk memfilter chunk berdasarkan kata kunci dari pertanyaan\n",
        "def filter_chunks_by_keywords(question, chunks):\n",
        "    keywords = extract_keywords(question)\n",
        "\n",
        "    if not keywords:  # Jika tidak ada kata kunci, gunakan semua chunk\n",
        "        return chunks\n",
        "\n",
        "    filtered_chunks = [chunk for chunk in chunks if any(keyword.lower() in chunk.lower() for keyword in keywords)]\n",
        "    return filtered_chunks if filtered_chunks else chunks  # Jika kosong, tetap gunakan semua chunk\n",
        "\n",
        "# Fungsi utama untuk menjawab pertanyaan dengan FAISS IVF\n",
        "def answer_question(question, chunks, index_faiss, embedder, cross_encoder_model, top_n=3):\n",
        "\n",
        "    filtered_chunks = filter_chunks_by_keywords(question, chunks)\n",
        "    if not filtered_chunks:\n",
        "        return \"Maaf, saya tidak dapat menemukan informasi yang sesuai.\"\n",
        "\n",
        "    # Embedding hanya untuk pertanyaan\n",
        "    question_embedding = embedder.encode([question], convert_to_numpy=True)\n",
        "\n",
        "    # Atur nprobe sebelum mencari\n",
        "    index_faiss.nprobe = NPROBE\n",
        "\n",
        "    # Cari similarity dengan FAISS IVF\n",
        "    D, I = index_faiss.search(question_embedding, min(top_n * 2, len(chunks)))\n",
        "    candidates = [chunks[i] for i in I[0] if i < len(chunks)]  # Pastikan indeks valid\n",
        "\n",
        "    # Gunakan Cross-Encoder untuk memilih chunk terbaik\n",
        "    pairs = [(question, chunk) for chunk in candidates]\n",
        "    scores = cross_encoder_model.predict(pairs)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
        "\n",
        "    return \"\\n\".join([candidates[i] for i in top_indices])\n",
        "\n",
        "def post_process_answer(answer):\n",
        "    sentences = sent_tokenize(answer)\n",
        "\n",
        "    # Hapus duplikasi dan urutkan kalimat agar lebih jelas\n",
        "    unique_sentences = list(dict.fromkeys(sentences))\n",
        "\n",
        "    # Format sebagai bullet list dengan memastikan keterbacaan\n",
        "    bulleted_list = \"\\n\".join([f\"* {sentence.strip()}\" for sentence in unique_sentences if len(sentence.strip()) > 10])\n",
        "\n",
        "    return bulleted_list"
      ],
      "metadata": {
        "id": "WHrZjdeiMLUC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING\n",
        "Chatbot menampilkan mengambil dan menampilkan 3 chunk teks yang paling mirip dengan pertanyaan pengguna sebagai jawaban."
      ],
      "metadata": {
        "id": "jZ1hT4h7fCl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 7. TESTING CHATBOT\n",
        "# ===============================\n",
        "\n",
        "# Contoh pertanyaan\n",
        "test_questions = [\n",
        "    \"Apakah Dana BOSP dapat digunakan untuk pengembangan sumber daya manusia?\",\n",
        "    \"Untuk apa saja Dana BOS Kinerja dapat digunakan?\",\n",
        "    \"Kapan laporan realisasi penggunaan Dana BOSP harus disampaikan?\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    raw_answer = answer_question(question, all_chunks, index, embedder, cross_encoder_model, top_n=3)\n",
        "    processed_answer = post_process_answer(raw_answer)\n",
        "\n",
        "    print(f\"\\n🔹 **Pertanyaan:** {question}\")\n",
        "    print(f\"🔸 **Jawaban:**\\n{processed_answer}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAASzWSmey1I",
        "outputId": "587223ba-caee-46b1-83d8-5aa6bcdeab14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 **Pertanyaan:** Apakah Dana BOSP dapat digunakan untuk pengembangan sumber daya manusia?\n",
            "🔸 **Jawaban:**\n",
            "* Rincian Komponen Penggunaan Dana BOS Kinerja Sekolah yang Melaksanakan Program Sekolah Penggerak a. Pengembangan sumber daya manusia merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan penguatan sumber daya manusia dalam rangka pelaksanaan Program Sekolah Penggerak, seperti: 1) identifikasi, pemetaan potensi dan kebutuhan pelatihan; 2) penguatan pelatihan griyaan (in house training) di Satuan Pendidikan; 3) penguatan komunitas belajar bagi kepala Satuan Pendidikan dan pendidik; 4) pelatihan bersama komunitas belajar; 5) pelaksanaan diskusi terpumpun bersama dengan guru SD kelas awal; 6) peningkatan kapasitas literasi digital; dan/ atau 7) kegiatan lainnya yang relevan dalam rangka pelaksanaan pengembangan sumber daya manusia.\n",
            "* Rincian Komponen Penggunaan Dana BOP PAUD Kinerja Sekolah yang Melaksanakan Program Sekolah Penggerak a. Pengembangan sumber daya manusia merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan penguatan sumber daya manusia dalam rangka pelaksanaan Program Sekolah Penggerak, seperti: 1) identifikasi, pemetaan potensi dan kebutuhan pelatihan; 2) penguatan pelatihan griyaan (in house training) di Satuan PAUD; 3) penguatan komunitas belajar bagi kepala Satuan PAUD dan pendidik; 4) pelatihan bersama komunitas belajar; 5) pelaksanaan diskusi terpumpun bersama dengan guru SD kelas awal; 6) peningkatan kapasitas literasi digital; dan/ atau 7) kegiatan lainnya yang relevan dalam rangka pelaksanaan pengembangan sumber daya manusia.\n",
            "* Komponen Penggunaan Dana BOS Kinerja untuk Sekolah yang Melaksanakan Program Sekolah Penggerak yang ditetapkan sebagai pelaksana Program pengimbasan adalah pembinaan dan pengembangan transformasi kepada satuan pendidikan lain untuk melakukan peningkatan mutu dalam hal pengembangan sumber daya manusia, pembelajaran kurikulum merdeka, digitalisasi sekolah, dan perencanaan berbasis data yang dilaksanakan dalam bentuk kegiatan seperti: a. pelatihan; b. penguatan komunitas belajar; dan c. pendampingan.\n",
            "\n",
            "\n",
            "🔹 **Pertanyaan:** Untuk apa saja Dana BOS Kinerja dapat digunakan?\n",
            "🔸 **Jawaban:**\n",
            "* Dana Bantuan Operasional Sekolah Kinerja yang selanjutnya disebut Dana BOS Kinerja adalah Dana BOS yang digunakan untuk peningkatan mutu pendidikan Satuan Pendidikan yang menyelenggarakan pendidikan dasar dan pendidikan menengah yang dinilai berkinerja baik.\n",
            "* Dana Bantuan Operasional Penyelenggaraan Pendidikan Anak Usia Dini Kinerja yang selanjutnya disebut Dana BOP PAUD Kinerja adalah Dana BOP PAUD yang digunakan untuk peningkatan mutu pendidikan Satuan Pendidikan yang menyelenggarakan pendidikan anak usia dini yang dinilai berkinerja baik.\n",
            "* Dana Bantuan Operasional Penyelenggaraan Pendidikan Kesetaraan Kinerja yang selanjutnya disebut Dana BOP Kesetaraan Kinerja adalah yang digunakan untuk peningkatan mutu pendidikan Satuan Pendidikan yang menyelenggarakan pendidikan kesetaraan program paket A, paket B, dan paket C yang dinilai berkinerja baik.\n",
            "\n",
            "\n",
            "🔹 **Pertanyaan:** Kapan laporan realisasi penggunaan Dana BOSP harus disampaikan?\n",
            "🔸 **Jawaban:**\n",
            "* (2) Penyampaian laporan realisasi penggunaan Dana BOSP sebagaimana dimaksud pada ayat (1) dilaksanakan paling lambat: a. tanggal Juli tahun anggaran berkenaan untuk laporan realisasi pengunaan Dana BOP PAUD Reguler, Dana BOS Reguler, atau Dana BOP Kesetaraan Reguler tahap I yang ada di Satuan Pendidikan; dan b. tanggal Januari tahun anggaran berikutnya untuk laporan realisasi keseluruhan penggunaan Dana BOSP yang diterima dalam satu tahun anggaran.\n",
            "* (2) Laporan realisasi keseluruhan sebagaimana dimaksud dalam dan laporan realisasi minimal % (lima puluh persen) penggunaan Dana BOP PAUD Reguler, Dana BOS Reguler, atau Dana BOP Kesetaraan Reguler yang diterima pada tahap I menjadi dasar penyaluran tahap II tahun anggaran berkenaan.\n",
            "* Ketentuan ayat (1) diubah, sehingga berbunyi sebagai berikut: (1) Menteri, gubernur, dan bupati/wali kota melakukan pemantauan dan evaluasi sesuai dengan kewenangannya.\n",
            "* Di antara dan disisipkan (satu) Pasal, yakni a sehingga berbunyi sebagai berikut: a (1) Laporan realisasi penggunaan Dana BOSP tahun sebelumnya digunakan sebagai dasar penyaluran tahap I tahun berkenaan.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}