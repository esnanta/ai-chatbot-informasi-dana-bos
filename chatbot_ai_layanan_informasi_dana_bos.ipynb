{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "1. This system that relies on semantic similarity. It finds the text in the document that is most similar to the user's question.\n",
        "2. If the user's question doesn't closely resemble the way the information is expressed in the document, the system may not find the correct answer.\n",
        "3. Basic Functionality covers:\n",
        "    * Extract text from PDF documents.\n",
        "    * Perform semantic search to find relevant chunks of text.\n",
        "    * Clean the output to remove unwanted content.\n",
        "    * Provide an answer to the user's question (even if the answer is not always perfect).\n",
        "\n"
      ],
      "metadata": {
        "id": "B9lCgwJuNeHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Development\n",
        "1. Clarifying Expectation, example :\n",
        "    * Chatbot: \"Dana BOS digunakan untuk membiayai kegiatan operasional sekolah. Apakah Anda ingin mengetahui contoh kegiatan operasional yang dapat dibiayai oleh Dana BOS?\"\n",
        "2. Provide a list of example questions that the user can ask. This shows them the types of questions the chatbot is good at answering. Example:\n",
        "    * Apa saja syarat pengajuan Dana BOS?\n",
        "    * Bagaimana cara melaporkan penggunaan Dana BOS?\n",
        "    * Sebutkan contoh kegiatan yang dapat dibiayai oleh Dana BOS.\n",
        "3. Keyword Suggestions: As the user types their question, suggest relevant keywords that they can include to make their question more specific.\n",
        "4. Intent Recognition (Advanced): Implement a simple intent recognition system. This would analyze the user's question and try to identify the intent behind it (e.g., \"find allowed uses,\" \"find reporting requirements\"). Based on the intent, the chatbot could automatically rephrase the question to be more targeted. This requires more advanced natural language processing techniques.\n",
        "5. Expand the Training Data (If Possible): If you have the ability to add more data to the system, try to find documents that explicitly list the allowed uses of Dana BOS in a clear and structured way. This will make it easier for the semantic search to find the right information.\n",
        "6. Hybrid Approach (Advanced): Consider combining this semantic search approach with a more traditional keyword-based search. If the semantic search fails to find a good answer, the chatbot could fall back to a keyword search to find any relevant documents and present them to the user."
      ],
      "metadata": {
        "id": "1uA6_DXcA7dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "uMo4iKw2NpWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omTDcRywLsNw",
        "outputId": "666fdbc9-9a9d-4723-c9f7-d157ffb0b7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "!pip install pymupdf nltk sastrawi transformers sentence-transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import fitz\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import drive\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download resource NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Gathering"
      ],
      "metadata": {
        "id": "8SFcHCpwORjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Gathering\n",
        "# ===============================\n",
        "# 1. MOUNT GOOGLE DRIVE & CEK FILES\n",
        "# ===============================\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path ke direktori penyimpanan file PDF\n",
        "pdf_dir = \"/content/drive/My Drive/Colab Notebooks/AI Chatbot Berbasis Regulasi\"\n",
        "\n",
        "# Cek apakah direktori ada\n",
        "if not os.path.exists(pdf_dir):\n",
        "    raise FileNotFoundError(f\"Direktori {pdf_dir} tidak ditemukan! Periksa kembali path-nya.\")\n",
        "else:\n",
        "    print(f\"Direktori ditemukan! Daftar file PDF: {os.listdir(pdf_dir)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUAjLu-hOUiX",
        "outputId": "eb411440-10fe-4962-b450-766836f84055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Direktori ditemukan! Daftar file PDF: ['Permendikbudriset No. 63 Tahun 2023.pdf', 'Untitled folder', 'extracted_text.json', 'processed_text.json', 'summarized_text.json', 'embeddings.npy', 'chunks.json', 'cleaned_texts.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 2. EKSTRAKSI TEKS DARI FILE PDF\n",
        "# ===============================\n",
        "\n",
        "# --- PDF Text Extraction ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
        "pdf_texts = {}\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
        "    try:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        pdf_texts[pdf_file] = text\n",
        "        print(f\"Extracted text from: {pdf_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_file}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfX2D-2Qfsf",
        "outputId": "db63280b-ebb4-413f-c88a-433fcf415a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text from: Permendikbudriset No. 63 Tahun 2023.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "AQ-aw2LwUTW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 3. PREPROCESSING TEKS\n",
        "# ===============================\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by removing extra newlines and spaces, URLs, specific phrases, and leading numbers.\"\"\"\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Prevent incorrect splitting: \"Pasal 17.\" -> \"Pasal 17 \"\n",
        "    text = re.sub(r'Pasal (\\d+)\\.\\s', r'Pasal \\1 ', text)\n",
        "    text = re.sub(r'Ayat \\((\\d+[a-z]?)\\)\\.\\s', r'Ayat (\\1) ', text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.IGNORECASE)  # Remove URLs\n",
        "    text = re.sub(r'jdih\\.kemdikbud\\.go\\.id', '', text, flags=re.IGNORECASE)  # Remove specific website\n",
        "    text = re.sub(r'OSP untuk operasional -3-', '', text)  # Remove specific phrase\n",
        "    text = re.sub(r'\\b\\d+\\.\\s*', '', text)  # Remove leading numbers like \"5. \"\n",
        "    text = re.sub(r'\\s-\\s\\d+\\s-\\s', ' ', text)  # Remove '- 4 -' pattern\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_texts = {pdf: clean_text(text) for pdf, text in pdf_texts.items()}"
      ],
      "metadata": {
        "id": "KHaUz08k3Fbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 4. CHUNKING TEKS\n",
        "# ===============================\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    \"\"\"Splits text into smaller chunks.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "for pdf, text in cleaned_texts.items():\n",
        "    chunks = chunk_text(text)\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "print(f\"Total chunks: {len(all_chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka9AdmeJHERa",
        "outputId": "af864acc-2a87-42b3-fbdc-5b95c66a2f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENISASI TEKS"
      ],
      "metadata": {
        "id": "_TRngQpOU0vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 5. TOKENISASI TEKS & EMBEDDING\n",
        "# ===============================\n",
        "\n",
        "# Load Sentence Transformer model (multilingual)\n",
        "model_name = 'firqaaa/indo-sentence-bert-base'  # Replace with the actual model\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Generate embeddings for the chunks\n",
        "embeddings = model.encode(all_chunks)"
      ],
      "metadata": {
        "id": "PL7UayJwU2Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING DATA"
      ],
      "metadata": {
        "id": "tNdrpdq1VEse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 6. SAVING DATA\n",
        "# ===============================\n",
        "\n",
        "# Define file paths for saving data\n",
        "embeddings_file = os.path.join(pdf_dir, \"embeddings.npy\")  # Path to save embeddings\n",
        "chunks_file = os.path.join(pdf_dir, \"chunks.json\")  # Path to save chunks\n",
        "cleaned_texts_file = os.path.join(pdf_dir, \"cleaned_texts.json\") # Path to save cleaned texts\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Saving the SentenceTransformer Model (NOT NECESSARY, SEE COMMENTS)\n",
        "# ------------------------------------------------------------------\n",
        "# As discussed, saving the SentenceTransformer model itself is not necessary\n",
        "# because you can simply load it from the Hugging Face Model Hub using the model_name.\n",
        "# Saving the model weights would take up a lot of space and is not required in this case.\n",
        "\n",
        "# --------------------------------------\n",
        "# 2. Saving the Embeddings\n",
        "# --------------------------------------\n",
        "try:\n",
        "    np.save(embeddings_file, embeddings)\n",
        "    print(f\"Embeddings saved to: {embeddings_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving embeddings: {e}\")\n",
        "\n",
        "# --------------------------------------\n",
        "# 3. Saving the Chunks of Text\n",
        "# --------------------------------------\n",
        "try:\n",
        "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Chunks saved to: {chunks_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving chunks: {e}\")\n",
        "\n",
        "# --------------------------------------\n",
        "# 4. Saving the Cleaned PDF Texts\n",
        "# --------------------------------------\n",
        "try:\n",
        "    with open(cleaned_texts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cleaned_texts, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Cleaned texts saved to: {cleaned_texts_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving cleaned texts: {e}\")"
      ],
      "metadata": {
        "id": "xCHxhNuqVGav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d5b303-2bd0-4437-b370-9359de0cbd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to: /content/drive/My Drive/Colab Notebooks/AI Chatbot Berbasis Regulasi/embeddings.npy\n",
            "Chunks saved to: /content/drive/My Drive/Colab Notebooks/AI Chatbot Berbasis Regulasi/chunks.json\n",
            "Cleaned texts saved to: /content/drive/My Drive/Colab Notebooks/AI Chatbot Berbasis Regulasi/cleaned_texts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "jZ1hT4h7fCl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 7. TESTING CHATBOT\n",
        "# ===============================\n",
        "\n",
        "# --- Question Answering ---\n",
        "def answer_question(question, embeddings, chunks, top_n=3):\n",
        "    \"\"\"Answers a question based on the text chunks.\"\"\"\n",
        "    question_embedding = model.encode([question])[0]\n",
        "    similarities = cosine_similarity([question_embedding], embeddings)[0]\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
        "\n",
        "    # Debugging: Print the top chunks\n",
        "    print(\"Top Chunks before post-processing:\")\n",
        "    for i in top_indices:\n",
        "        print(f\"Chunk {i}: {chunks[i]}\\n---\")\n",
        "\n",
        "    context = \"\\n\".join([chunks[i] for i in top_indices])\n",
        "\n",
        "    return context\n",
        "\n",
        "def post_process_answer(answer):\n",
        "    \"\"\"Formats the answer into a bulleted list.\"\"\"\n",
        "    # Split the answer into sentences\n",
        "    sentences = sent_tokenize(answer)\n",
        "\n",
        "    # Create a bulleted list from the sentences\n",
        "    bulleted_list = \"\\n\".join([f\"* {sentence.strip()}\" for sentence in sentences])\n",
        "\n",
        "    return bulleted_list\n",
        "\n",
        "# --- Example Usage ---\n",
        "question = \"Jelaskan komponen pembinaan dan pengembangan prestasi?\"  # More focused question\n",
        "raw_answer = answer_question(question, embeddings, all_chunks, top_n=3)\n",
        "processed_answer = post_process_answer(raw_answer)\n",
        "\n",
        "print(f\"Pertanyaan: {question}\")\n",
        "print(f\"Jawaban:\\n{processed_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAASzWSmey1I",
        "outputId": "f153031e-a450-46b1-cd94-8314e478a1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Chunks before post-processing:\n",
            "Chunk 54: b. Pelatihan dan Pengembangan Talenta merupakan komponen yang digunakan untuk pembiayaan kegiatan pelaksanaan pelatihan dan pengembangan talenta Peserta Didik, seperti: 1) Peningkatan kapasitas peserta didik berprestasi; 2) peningkatan kapasitas bagi Peserta Didik berprestasi untuk melanjutkan karier belajar; 3) Penyediaan sarana penunjang ketalentaan; 4) Penyelenggaraan kompetisi internal sekolah; 5) Pembinaan dan partisipasi kompetisi eksternal; dan/atau 6) kegiatan lainnya yang relevan dalam rangka pelatihan dan pengembangan talenta.\n",
            "---\n",
            "Chunk 55: c. Pengembangan Manajemen dan Ekosistem merupakan komponen yang digunakan untuk kegiatan pengembangan manajemen dan ekosistem sekolah, seperti: 1) Peningkatan kapasitas guru dalam rangka asesmen dan pemetaan talenta; 2) Pengembangan kemitraan; 3) Pengembangan strategi manajemen talenta sekolah; 4) Perencanaan berbasis potensi ketalentaan sekolah; 5) pengelolaan data dan informasi talenta; dan/atau 6) kegiatan lainnya yang relevan dalam rangka pengembangan manajemen dan ekosistem.\n",
            "---\n",
            "Chunk 56: d. Pembinaan dan pengembangan prestasi satuan pendidikan melalui program pengimbasan untuk sekolah pengimbas merupakan komponen yang digunakan untuk pembiayaan kegiatan pembinaan dan pengembangan kepada satuan pendidikan yang belum berprestasi, seperti: 1) pengembangan kapasitas sumber daya manusia talenta sekolah imbas; 2) kegiatan pemberian pendampingan dan layanan konsultasi pelaksanaan pengembangan program manajemen talenta bagi sekolah imbas; 3) pengembangan talenta sekolah imbas melalui kemitraan; 4) penyelenggaraan kompetisi lokal antar sekolah bersama sekolah imbas; dan/atau 5) kegiatan lain yang relevan dalam rangka pembinaan dan pengembangan prestasi sekolah imbas.\n",
            "---\n",
            "Pertanyaan: Jelaskan komponen pembinaan dan pengembangan prestasi?\n",
            "Jawaban:\n",
            "* b. Pelatihan dan Pengembangan Talenta merupakan komponen yang digunakan untuk pembiayaan kegiatan pelaksanaan pelatihan dan pengembangan talenta Peserta Didik, seperti: 1) Peningkatan kapasitas peserta didik berprestasi; 2) peningkatan kapasitas bagi Peserta Didik berprestasi untuk melanjutkan karier belajar; 3) Penyediaan sarana penunjang ketalentaan; 4) Penyelenggaraan kompetisi internal sekolah; 5) Pembinaan dan partisipasi kompetisi eksternal; dan/atau 6) kegiatan lainnya yang relevan dalam rangka pelatihan dan pengembangan talenta.\n",
            "* c. Pengembangan Manajemen dan Ekosistem merupakan komponen yang digunakan untuk kegiatan pengembangan manajemen dan ekosistem sekolah, seperti: 1) Peningkatan kapasitas guru dalam rangka asesmen dan pemetaan talenta; 2) Pengembangan kemitraan; 3) Pengembangan strategi manajemen talenta sekolah; 4) Perencanaan berbasis potensi ketalentaan sekolah; 5) pengelolaan data dan informasi talenta; dan/atau 6) kegiatan lainnya yang relevan dalam rangka pengembangan manajemen dan ekosistem.\n",
            "* d. Pembinaan dan pengembangan prestasi satuan pendidikan melalui program pengimbasan untuk sekolah pengimbas merupakan komponen yang digunakan untuk pembiayaan kegiatan pembinaan dan pengembangan kepada satuan pendidikan yang belum berprestasi, seperti: 1) pengembangan kapasitas sumber daya manusia talenta sekolah imbas; 2) kegiatan pemberian pendampingan dan layanan konsultasi pelaksanaan pengembangan program manajemen talenta bagi sekolah imbas; 3) pengembangan talenta sekolah imbas melalui kemitraan; 4) penyelenggaraan kompetisi lokal antar sekolah bersama sekolah imbas; dan/atau 5) kegiatan lain yang relevan dalam rangka pembinaan dan pengembangan prestasi sekolah imbas.\n"
          ]
        }
      ]
    }
  ]
}